{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Forward propagation is the process by which the input data is passed through the neural network to generate an output. It involves the following steps:\n",
    "1. **Input Layer**: The input data is fed into the input layer of the neural network.\n",
    "2. **Hidden Layers**: The data is then passed through one or more hidden layers. Each neuron in a hidden layer takes a weighted sum of the inputs, applies an activation function, and passes the result to the next layer.\n",
    "3. **Output Layer**: Finally, the data reaches the output layer, where the final prediction is made.\n",
    "\n",
    "### Back Propagation\n",
    "Back propagation is the process by which the neural network learns from the errors in its predictions. It involves the following steps:\n",
    "1. **Calculate Error**: The error is calculated by comparing the predicted output with the actual output.\n",
    "2. **Output Layer**: The error is propagated back from the output layer to the hidden layers. The gradients of the error with respect to the weights are calculated.\n",
    "3. **Hidden Layers**: The gradients are used to update the weights of the neurons in the hidden layers.\n",
    "4. **Input Layer**: The process continues until the input layer is reached, and all weights are updated to minimize the error.\n",
    "\n",
    "Both forward and back propagation are essential for training neural networks and improving their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Their Use Cases\n",
    "\n",
    "Activation functions are crucial in neural networks as they introduce non-linearity, allowing the network to learn complex patterns. Here are some commonly used activation functions and their use cases:\n",
    "\n",
    "1. **Sigmoid Function**\n",
    "    - **Formula**: $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "    - **Use Case**: Often used in the output layer of binary classification problems. It squashes the output to a range between 0 and 1.\n",
    "\n",
    "2. **Tanh (Hyperbolic Tangent) Function**\n",
    "    - **Formula**: $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "    - **Use Case**: Commonly used in hidden layers. It outputs values between -1 and 1, which can help in centering the data and making the optimization process easier.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit) Function**\n",
    "    - **Formula**: $ \\text{ReLU}(x) = \\max(0, x) $\n",
    "    - **Use Case**: Widely used in hidden layers of deep neural networks. It helps in mitigating the vanishing gradient problem and is computationally efficient.\n",
    "\n",
    "4. **Leaky ReLU Function**\n",
    "    - **Formula**: $ \\text{Leaky ReLU}(x) = \\max(0.01x, x) $\n",
    "    - **Use Case**: Similar to ReLU but allows a small gradient when the unit is not active. It helps in addressing the \"dying ReLU\" problem.\n",
    "\n",
    "5. **Softmax Function**\n",
    "    - **Formula**: $ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $\n",
    "    - **Use Case**: Used in the output layer of multi-class classification problems. It converts logits into probabilities that sum to 1.\n",
    "\n",
    "Choosing the right activation function depends on the specific problem and the architecture of the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/f89ba7c0-4a51-4440-8323-baad898c5af0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation and Gradient Descent\n",
    "\n",
    "Backpropagation uses gradient descent to minimize the error in neural networks. Here's how it works:\n",
    "\n",
    "1. **Gradient Descent**: This is an optimization algorithm used to minimize the loss function. It involves calculating the gradient of the loss function with respect to the weights and updating the weights in the opposite direction of the gradient.\n",
    "\n",
    "2. **Backpropagation**: During backpropagation, the gradient of the loss function is calculated for each weight by propagating the error backward through the network. The weights are then updated using gradient descent.\n",
    "\n",
    "The combination of backpropagation and gradient descent allows the neural network to learn from the errors and improve its predictions over time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
