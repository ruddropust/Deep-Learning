{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "An activation function decides wheter a neural should be activated or not.This means that it will decided wheter the neuron input to the netwok is important or not in the process of prediction using simpler mathematical operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of Activation Functions\n",
    "\n",
    "1. **Binary Step**: This function activates the neuron if the input is above a certain threshold.\n",
    "2. **Linear**: This function is a straight line where the activation is proportional to the input.\n",
    "3. **Non-Linear**: These functions introduce non-linearity into the network, allowing it to learn complex patterns. Examples include Sigmoid, Tanh, and ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Binary Step Activation Function\n",
    "\n",
    "The Binary Step Activation Function is defined as:\n",
    "\n",
    "This function activates the neuron if the input is above a certain threshold (usually 0). It is ``not commonly used in practice because it does not allow for backpropagation``, as the gradient is zero for all inputs.\n",
    "The equation in LaTeX notation is:\n",
    "$$\n",
    "f(x) = \\begin{cases} \n",
    "1 & \\text{if } x \\geq 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://iq.opengenus.org/content/images/2021/11/step-func-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Linear Activation Function\n",
    "\n",
    "The Linear Activation Function is defined as:\n",
    "$$\n",
    "f(x) = x\n",
    "$$\n",
    "\n",
    "This function is a straight line where the activation is proportional to the input. It is commonly used in ``simple regression problems``. However, it has limitations in neural networks because it does not introduce non-linearity, which is essential for learning complex patterns.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:867/1*tldIgyDQWqm-sMwP7m3Bww.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Non-Linear Activation Functions\n",
    "\n",
    "\n",
    "Non-linear activation functions introduce non-linearity into the network, allowing it to learn complex patterns. These functions enable the network to approximate any function and are essential for deep learning. Common non-linear activation functions include:\n",
    "\n",
    "1. **Sigmoid**: The Sigmoid function outputs a value between 0 and 1, which can be interpreted as a probability. It is defined as:\n",
    "    $$\n",
    "    f(x) = \\frac{1}{1 + e^{-x}}\n",
    "    $$\n",
    "    ![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n",
    "\n",
    "\n",
    "\n",
    "    The Sigmoid function is commonly used in binary classification problems. However, it can suffer from the vanishing gradient problem, where the gradients become very small, slowing down the training process.\n",
    "    ``This function also differentiable which is good for gradent descent``\n",
    "    - It is the only function that appears in its derivative.\n",
    "    - It is differentiable at every point, which helps in the effective computation of gradients during backpropagation.\n",
    "    - When updating weights and biases using gradient descent, if the gradients are too small, the updates to weights and biases become insignificant, slowing down or even stopping learning.\n",
    "    - It's not 0 center\n",
    "\n",
    "2. **Tanh**: The Tanh function outputs a value between -1 and 1. It is defined as:\n",
    "    $$\n",
    "    \\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "    $$\n",
    "\n",
    "    ![Tanh](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTCQepq4XYphs3xIY0HrGXtMTxSSR2U5lyPp6w78XtxdmQ9d1O7hJSm_0TbNv-9_eZ0eMU&usqp=CAU)\n",
    "    - It's mostly used in **RNN**\n",
    "    - It's  0 center\n",
    "    - it can't suffer from the vanishing gradient problem\n",
    "    - good for binary classification\n",
    "\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)**: The ReLU function outputs the input directly if it is positive; otherwise, it outputs zero. It is defined as:\n",
    "    $$\n",
    "    f(x) = \\max(0, x)\n",
    "    $$\n",
    "    ![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/1200px-Rectifier_and_softplus_functions.svg.png)\n",
    "\n",
    "    - **When to Use ReLU:** ReLU is widely used in hidden layers of neural networks due to its simplicity and effectiveness. It helps to mitigate the vanishing gradient problem, allowing for faster and more efficient training. However, it can suffer from the \"dying ReLU\" problem, where neurons can become inactive and only output zero. To address this, variants like Leaky ReLU and Parametric ReLU can be used.\n",
    "    - differentiable\n",
    "4. **Softmax**: Softmax is another commonly used activation function, especially in the output layer of classification problems. It converts logits into probabilities by applying the exponential function and normalizing the results. It is defined as:\n",
    "$$\n",
    "f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "![softmax](https://cdn.botpenguin.com/assets/website/Softmax_Function_07fe934386.png)\n",
    "\n",
    "The Softmax activation function is commonly used in the output layer of neural networks for multi-class classification problems. It converts the raw output scores (logits) of the network into probabilities, which sum to 1. This makes it easier to interpret the output as the probability of each class.\n",
    "\n",
    "Here is a brief explanation of when and why to use the Softmax function:\n",
    "\n",
    "- **Multi-Class Classification:** Softmax is ideal for problems where the goal is to classify an input into one of several classes. For example, in image classification tasks where an image needs to be classified as one of many possible categories (e.g., cat, dog, bird).\n",
    "\n",
    "- **Probability Distribution:** Softmax outputs a probability distribution over all possible classes. This means each output value is between 0 and 1, and the sum of all output values is 1. This property is useful for interpreting the model's confidence in each class.\n",
    "\n",
    "- **Cross-Entropy Loss:** When using Softmax in the output layer, it is common to pair it with the cross-entropy loss function. This combination is effective for training models to minimize the difference between the predicted probability distribution and the true distribution (one-hot encoded labels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "guidelines for choosing the right activation function for the output layer based on the type of prediction problem:\n",
    "\n",
    "- **Regression:** Use a Linear Activation Function.\n",
    "\n",
    "- **Binary Classification:** Use a Sigmoid/Logistic Activation Function.\n",
    "\n",
    "- **Multiclass Classification:** Use Softmax.\n",
    "\n",
    "- **Multilabel Classification:** Use Sigmoid.\n",
    "- **Convolutional Neural Network (CNN):** ReLU activation function.\n",
    "\n",
    "- **Recurrent Neural Network (RNN):** Tanh and/or Sigmoid activation function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
