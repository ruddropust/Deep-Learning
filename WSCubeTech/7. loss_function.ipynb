{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "A loss function measures how well the model's predictions match the actual data. It is a crucial component in training machine learning models, as it guides the optimization process. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "#### Example: Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error (MSE) is calculated as the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of data points  \n",
    "- $y_i$ is the actual value  \n",
    "- $\\hat{y}_i$ is the predicted value  \n",
    "\n",
    "#### Example: Cross-Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss is commonly used for classification tasks. It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of data points  \n",
    "- $y_i$ is the actual binary label (0 or 1)  \n",
    "- $\\hat{y}_i$ is the predicted probability  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function vs. Cost Function in Machine Learning\n",
    "\n",
    "In machine learning, both **loss** and **cost** functions are used to measure how well a model's predictions match the actual data. However, they are used in slightly different contexts:\n",
    "\n",
    "#### Loss Function:\n",
    "- The loss function measures the error for a **single training example**.\n",
    "- It quantifies how well or poorly the model performs on a **single instance**.\n",
    "- Common examples include **Mean Squared Error (MSE)** for regression and **Cross-Entropy Loss** for classification.\n",
    "\n",
    "#### Cost Function:\n",
    "- The cost function, also known as the **objective function**, measures the **average error** over the **entire training dataset**.\n",
    "- It is essentially the **average of the loss function** over all training examples.\n",
    "- The goal of training a model is to **minimize the cost function**.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's consider a simple **linear regression** problem:\n",
    "\n",
    "#### **Loss Function**  \n",
    "For a **single training example** $(x_i, y_i)$, the **Mean Squared Error (MSE)** loss function can be defined as:\n",
    "\n",
    "$$\n",
    "L(y_i, \\hat{y_i}) = (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $y_i$ is the actual value  \n",
    "- $\\hat{y_i}$ is the predicted value  \n",
    "\n",
    "#### **Cost Function**  \n",
    "For the **entire training dataset** with $n$ examples, the cost function is the **average of the loss function** over all examples:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, \\hat{y_i})\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\theta$ represents the model parameters  \n",
    "\n",
    "### **Summary**\n",
    "- The **loss function** evaluates the error for a **single instance**.  \n",
    "- The **cost function** evaluates the **average error** over the entire dataset.  \n",
    "- The objective of model training is to **minimize the cost function**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Loss Functions for Regression\n",
    "\n",
    "When dealing with regression tasks, the goal is to predict continuous values. Here are some commonly used loss functions for regression:\n",
    "\n",
    "#### 1. Mean Squared Error (MSE)\n",
    "Measures the average of the squares of the errors between predicted and actual values.\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "#### 2. Mean Absolute Error (MAE)\n",
    "Measures the average of the absolute differences between predicted and actual values.\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "#### 3. Huber Loss\n",
    "Combines the best properties of MSE and MAE, making it less sensitive to outliers in data.\n",
    "$$\n",
    "L_\\delta(a) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{2}a^2 & \\text{for } |a| \\leq \\delta \\\\\n",
    "\\delta (|a| - \\frac{1}{2}\\delta) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Summary\n",
    "- **MSE** and **MAE** are commonly used for regression tasks.\n",
    "- **Huber Loss** is a robust loss function for regression that is less sensitive to outliers.\n",
    "\n",
    "### Types of Loss Functions for Classification\n",
    "\n",
    "When dealing with classification tasks, the goal is to predict discrete class labels. Here are some commonly used loss functions for classification:\n",
    "\n",
    "#### 1. Binary Cross-Entropy Loss\n",
    "Used for binary classification tasks where the output is a probability value between 0 and 1.\n",
    "$$\n",
    "\\text{Binary Cross-Entropy Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "#### 2. Categorical Cross-Entropy Loss\n",
    "Used for multi-class classification tasks where the output is a probability distribution over multiple classes.\n",
    "$$\n",
    "\\text{Categorical Cross-Entropy Loss} = -\\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of data points\n",
    "- $C$ is the number of classes\n",
    "- $y_{i,c}$ is the binary indicator (0 or 1) if class label $c$ is the correct classification for observation $i$\n",
    "- $\\hat{y}_{i,c}$ is the predicted probability of observation $i$ being in class $c$\n",
    "\n",
    "### Summary\n",
    "- **Binary Cross-Entropy Loss** is used for binary classification tasks.\n",
    "- **Categorical Cross-Entropy Loss** is used for multi-class classification tasks.\n",
    "\n",
    "### Types of Loss Functions for Autoencoders\n",
    "\n",
    "Autoencoders are a type of neural network used to learn efficient representations of data, typically for the purpose of dimensionality reduction or feature learning. One commonly used loss function for autoencoders is the Kullback-Leibler (KL) Divergence.\n",
    "\n",
    "#### KL Divergence\n",
    "Measures the difference between two probability distributions. It is often used in variational autoencoders (VAEs) to measure the divergence between the learned latent variable distribution and a prior distribution.\n",
    "$$\n",
    "\\text{KL}(P \\parallel Q) = \\sum_{i} P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P$ is the true distribution\n",
    "- $Q$ is the approximate distribution\n",
    "\n",
    "### Summary\n",
    "- **KL Divergence** is commonly used in variational autoencoders to measure the difference between the learned latent variable distribution and a prior distribution.\n",
    "\n",
    "### Types of Loss Functions for Generative Adversarial Networks (GANs)\n",
    "\n",
    "Generative Adversarial Networks (GANs) consist of two neural networks, a generator and a discriminator, that are trained simultaneously. The discriminator's loss function is crucial for training GANs. Here are some commonly used loss functions for GANs:\n",
    "\n",
    "#### 1. Discriminator Loss\n",
    "The discriminator's loss measures how well it can distinguish between real and generated (fake) data. It is typically defined as the sum of the binary cross-entropy losses for real and fake data.\n",
    "$$\n",
    "\\text{Discriminator Loss} = -\\frac{1}{2} \\left( \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}} [\\log (1 - D(G(z)))] \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $D(x)$ is the discriminator's output for real data $x$\n",
    "- $D(G(z))$ is the discriminator's output for generated data $G(z)$\n",
    "- $p_{\\text{data}}$ is the distribution of real data\n",
    "- $p_{z}$ is the distribution of the generator's input noise\n",
    "\n",
    "#### 2. Minimax GAN Loss\n",
    "The minimax loss is the original loss function proposed for GANs. The generator aims to minimize the probability that the discriminator correctly identifies generated data as fake, while the discriminator aims to maximize this probability.\n",
    "$$\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}} [\\log (1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $G$ is the generator\n",
    "- $D$ is the discriminator\n",
    "- $V(D, G)$ is the value function representing the minimax game between $G$ and $D$\n",
    "\n",
    "### Summary\n",
    "- **Discriminator Loss** measures how well the discriminator can distinguish between real and generated data.\n",
    "- **Minimax GAN Loss** is the original loss function for GANs, involving a minimax game between the generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Loss Functions for Object Detection: Focal Loss\n",
    "\n",
    "Object detection tasks involve identifying and localizing objects within an image. One commonly used loss function for object detection is Focal Loss.\n",
    "\n",
    "#### Focal Loss\n",
    "Focal Loss is designed to address the class imbalance problem in object detection tasks by down-weighting the loss assigned to well-classified examples. This helps the model focus more on hard, misclassified examples.\n",
    "\n",
    "The Focal Loss is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Focal Loss} = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p_t$ is the predicted probability for the true class\n",
    "- $\\alpha_t$ is a weighting factor for the class\n",
    "- $\\gamma$ is a focusing parameter that adjusts the rate at which easy examples are down-weighted\n",
    "\n",
    "### Summary\n",
    "- **Focal Loss** is used in object detection tasks to address class imbalance by down-weighting well-classified examples and focusing more on hard, misclassified examples.\n",
    "\n",
    "### Types of Loss Functions for Word Embeddings: Triplet Loss\n",
    "\n",
    "Word embeddings are vector representations of words that capture their meanings, syntactic properties, and relationships with other words. One commonly used loss function for training word embeddings is Triplet Loss.\n",
    "\n",
    "#### Triplet Loss\n",
    "Triplet Loss is used to ensure that words with similar meanings are closer in the embedding space, while words with different meanings are farther apart. It works by comparing the distances between an anchor word, a positive word (similar in meaning), and a negative word (different in meaning).\n",
    "\n",
    "The Triplet Loss is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Triplet Loss} = \\max(0, d(a, p) - d(a, n) + \\alpha)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $a$ is the anchor word\n",
    "- $p$ is the positive word\n",
    "- $n$ is the negative word\n",
    "- $d(a, p)$ is the distance between the anchor and positive words\n",
    "- $d(a, n)$ is the distance between the anchor and negative words\n",
    "- $\\alpha$ is a margin that is enforced between positive and negative pairs\n",
    "\n",
    "### Summary\n",
    "- **Triplet Loss** is used in training word embeddings to ensure that similar words are closer in the embedding space, while dissimilar words are farther apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE (L1 Loss)\n",
    "If dataset consist outlier then ``not used MSE``. It's can be start ``wrong prediction``.`\n",
    "\n",
    "# MAE(L1 Loss)\n",
    "If dataset consist outlier then `used MSE`.It's not used because of not differentiable.\n",
    "![Image](https://github.com/user-attachments/assets/140b88ed-bd2b-4c4c-8c5f-00317fe4155d)\n",
    "\n",
    "# Huber Loss\n",
    "when dealing with datasets that`` contain outliers``, as it is ``less sensitive to outliers compared to MSE``. Huber Loss is differentiable and combines the best properties of MSE and MAE, making it a ``robust choice for regression`` tasks ``with outliers``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy/Log Loss\n",
    "when data is binary format(ex.cat,dog) then use it.\n",
    "Binary Cross-Entropy Loss is used when the target variable is binary, meaning it has two possible classes (e.g., cat and dog). It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "\n",
    "# Categorical Cross-entropy\n",
    "Categorical Cross-Entropy Loss is used when the target variable has multiple classes (e.g., cat, dog, and bird). It measures the performance of a classification model whose output is a probability distribution over multiple classes.\n",
    "\n",
    "`If many categories occure like more than 5 then not used Categorical Cross-entropy`\n",
    "\n",
    "# Sparse Categorical Cross-entropy\n",
    "Sparse Categorical Cross-Entropy Loss is used when the target variable has multiple classes, and the ``target labels are provided as integers rather than one-hot encoded vectors``. It is ``particularly useful when dealing with a large number of classes``, as it is ``more memory-efficient``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
