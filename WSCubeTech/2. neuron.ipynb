{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron is a specialized cell in the nervous system that transmits information through electrical and chemical signals. It consists of three main parts:\n",
    "\n",
    "1. **Cell Body (Soma)**: Contains the nucleus and other organelles, responsible for maintaining the cell's health.\n",
    "2. **Dendrites**: Branch-like structures that receive messages from other neurons and transmit them to the cell body.\n",
    "3. **Axon**: A long, thin structure that carries electrical impulses away from the cell body to other neurons, muscles, or glands.\n",
    "\n",
    "Neurons communicate with each other at junctions called synapses, where the axon terminal of one neuron meets the dendrite of another. This communication is essential for all nervous system functions, including sensation, thought, and movement.\n",
    "\n",
    "![Neuron](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQjcFuPQwM_qyYEzl_xzjEZ7MtecaGqwyvNUg&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "An activation function is a mathematical function used in artificial neural networks to determine the output of a neuron. It introduces non-linearity into the network, allowing it to learn and model complex data patterns. Common activation functions include:\n",
    "\n",
    "1. **Sigmoid**:  \n",
    "    $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$  \n",
    "    It outputs values between 0 and 1, making it suitable for binary classification.\n",
    "\n",
    "2. **Tanh**:  \n",
    "    $$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$  \n",
    "    It outputs values between -1 and 1, often used in hidden layers.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)**:  \n",
    "    $$\\text{ReLU}(x) = \\max(0, x)$$  \n",
    "    It outputs the input directly if it is positive; otherwise, it outputs zero. It is widely used due to its simplicity and effectiveness.\n",
    "\n",
    "4. **Leaky ReLU**:  \n",
    "    $$\n",
    "    \\text{Leaky ReLU}(x) = \n",
    "    \\begin{cases} \n",
    "    x & \\text{if } x > 0 \\\\\n",
    "    \\alpha x & \\text{if } x \\leq 0 \n",
    "    \\end{cases}\n",
    "    $$  \n",
    "    It allows a small, non-zero gradient when the unit is not active, addressing the \"dying ReLU\" problem.\n",
    "\n",
    "Activation functions play a crucial role in the performance and training of neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Deep Learning Networks\n",
    "\n",
    "Deep learning networks come in various architectures, each designed to handle specific types of data and tasks. Some of the most common types include:\n",
    "\n",
    "- **Perceptron**:\n",
    "    - The simplest type of artificial neural network.\n",
    "    - Consists of a single layer of neurons.\n",
    "    - Used for binary classification tasks.\n",
    "\n",
    "- **Multilayer Perceptron (MLP) also called ANN**:\n",
    "    - An extension of the perceptron with multiple layers.\n",
    "    - Consists of an input layer, one or more hidden layers, and an output layer.\n",
    "    - Capable of learning complex patterns through backpropagation.\n",
    "\n",
    "- **Radial Basis Function (RBF) Network**:\n",
    "    - Uses radial basis functions as activation functions.\n",
    "    - Consists of an input layer, a hidden layer with RBF neurons, and an output layer.\n",
    "    - Effective for function approximation and pattern recognition tasks.\n",
    "\n",
    "- **Feedforward Neural Networks (FNNs)**:\n",
    "    - The simplest type of artificial neural network.\n",
    "    - Consists of an input layer, one or more hidden layers, and an output layer.\n",
    "    - Information moves in one direction, from input to output.\n",
    "\n",
    "- **Convolutional Neural Networks (CNNs)**:\n",
    "    - Primarily used for image and video recognition tasks.\n",
    "    - Consists of convolutional layers, pooling layers, and fully connected layers.\n",
    "    - Captures spatial hierarchies in data through convolution operations.\n",
    "\n",
    "- **Recurrent Neural Networks (RNNs)**:\n",
    "    - Designed for sequential data, such as time series or natural language.\n",
    "    - Contains loops that allow information to persist, making them suitable for tasks like language modeling and translation.\n",
    "    - Variants include Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks.\n",
    "\n",
    "- **Generative Adversarial Networks (GANs)**:\n",
    "    - Consists of two networks: a generator and a discriminator.\n",
    "    - The generator creates fake data, while the discriminator evaluates its authenticity.\n",
    "    - Used for tasks like image generation, style transfer, and data augmentation.\n",
    "\n",
    "- **Autoencoders**:\n",
    "    - Used for unsupervised learning tasks, such as dimensionality reduction and anomaly detection.\n",
    "    - Consists of an encoder that compresses the input and a decoder that reconstructs it.\n",
    "    - Variants include Variational Autoencoders (VAEs) and Denoising Autoencoders.\n",
    "\n",
    "- **Transformer Networks**:\n",
    "    - Primarily used for natural language processing tasks.\n",
    "    - Relies on self-attention mechanisms to process input data.\n",
    "    - Forms the basis of models like BERT and GPT.\n",
    "\n",
    "Each type of deep learning network has its strengths and is chosen based on the specific requirements of the task at hand.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
