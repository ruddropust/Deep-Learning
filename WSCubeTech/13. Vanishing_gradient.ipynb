{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient Problem\n",
    "The vanishing gradient problem is a phenomenon that occurs during the training of artificial neural networks, particularly deep neural networks. It refers to the situation where the gradients of the loss function with respect to the weights become very small, effectively preventing the weights from updating significantly. This can lead to very slow convergence or even the complete halt of training. The problem is more pronounced in networks with many layers, where the gradients can exponentially decrease as they are propagated back through the network. This makes it difficult for the network to learn and improve its performance. Techniques such as using different activation functions (e.g., ReLU), weight initialization methods, and normalization techniques (e.g., batch normalization) are often employed to mitigate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/0aba62b0-2072-4f6a-b503-c1cc0f892997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem occurs during the training of deep neural networks, particularly when using gradient-based learning methods and backpropagation. It happens when the gradients of the loss function with respect to the weights become very small, effectively preventing the weights from updating. This can lead to the network learning very slowly or not at all.\n",
    "\n",
    "### Causes of Vanishing Gradient Problem:\n",
    "1. **Activation Functions**: Certain activation functions like the sigmoid or tanh can squash input values into a small range, causing gradients to diminish as they propagate back through the network.\n",
    "2. **Deep Networks**: In very deep networks, the repeated multiplication of small gradients can lead to an exponential decrease in gradient values as they move backward through the layers.\n",
    "\n",
    "### Solutions to Vanishing Gradient Problem:\n",
    "1. **ReLU Activation Function**: Using activation functions like ReLU (Rectified Linear Unit) which do not squash gradients.\n",
    "2. **Weight Initialization**: Proper initialization techniques like Xavier or He initialization can help maintain gradient magnitudes.\n",
    "3. **Batch Normalization**: Normalizing the inputs of each layer can help maintain gradient flow.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Example of vanishing gradient\n",
    "x = np.array([1, 2, 3])\n",
    "gradients = sigmoid_derivative(x)\n",
    "print(\"Gradients:\", gradients)\n",
    "```\n",
    "\n",
    "In this example, the gradients of the sigmoid function are very small, illustrating how the vanishing gradient problem can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
