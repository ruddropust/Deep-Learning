{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in Deep Learning\n",
    "\n",
    "Regularization is a technique used in deep learning to prevent overfitting by adding a penalty to the loss function. It helps improve the generalization of the model on unseen data. Below are some common regularization techniques:\n",
    "\n",
    "## 1. L1 and L2 Regularization\n",
    "- **L1 Regularization (Lasso)**: Adds the absolute value of weights to the loss function. It encourages sparsity in the model by driving some weights to zero.\n",
    "- **L2 Regularization (Ridge)**: Adds the squared value of weights to the loss function. It discourages large weights and helps in weight decay.\n",
    "\n",
    "### Loss Function with Regularization:\n",
    "$\\text{Loss} = \\text{Original Loss} + \\lambda \\sum_{i} |w_i| \\quad (\\text{L1})$\n",
    "\n",
    "$\\text{Loss} = \\text{Original Loss} + \\lambda \\sum_{i} w_i^2 \\quad (\\text{L2})$\n",
    "\n",
    "## 2. Dropout\n",
    "- Randomly drops a fraction of neurons during training to prevent co-adaptation of neurons.\n",
    "- Helps in reducing overfitting and improves generalization.\n",
    "\n",
    "### Dropout Example:\n",
    "If the dropout rate is 0.5, half of the neurons are randomly set to zero during each training iteration.\n",
    "\n",
    "## 3. Early Stopping\n",
    "- Stops training when the validation loss stops improving.\n",
    "- Prevents the model from overfitting to the training data.\n",
    "\n",
    "## 4. Data Augmentation\n",
    "- Increases the size of the training dataset by applying transformations like rotation, flipping, cropping, etc.\n",
    "- Helps the model generalize better by exposing it to varied data.\n",
    "\n",
    "## 5. Batch Normalization\n",
    "- Normalizes the inputs to each layer to have zero mean and unit variance.\n",
    "- Acts as a regularizer by reducing internal covariate shift and improving training speed.\n",
    "\n",
    "## 6. Weight Initialization\n",
    "- Proper initialization of weights can act as a form of implicit regularization.\n",
    "- Techniques like Xavier or He initialization help in stabilizing training.\n",
    "\n",
    "## 7. Ensemble Methods\n",
    "- Combines predictions from multiple models to reduce variance and improve robustness.\n",
    "- Examples include bagging, boosting, and stacking.\n",
    "\n",
    "By applying these techniques, deep learning models can achieve better performance on unseen data while avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs when a machine learning model learns the training data too well, including its noise and irrelevant details, instead of generalizing to unseen data. This results in a model that performs well on the training set but poorly on new, unseen data.\n",
    "\n",
    "### Key Reasons for Overfitting:\n",
    "1. **Model Complexity**:\n",
    "   - The model is too complex (e.g., too many layers, neurons, or parameters) relative to the amount of training data. This allows it to memorize the training data instead of learning general patterns.\n",
    "\n",
    "2. **Insufficient Training Data**:\n",
    "   - When the dataset is too small, the model has limited examples to learn from, making it prone to memorizing the data.\n",
    "\n",
    "3. **Noise in Data**:\n",
    "   - If the training data contains noise or outliers, the model may learn these irrelevant details, reducing its ability to generalize.\n",
    "\n",
    "4. **Lack of Regularization**:\n",
    "   - Regularization techniques like L1/L2 regularization, dropout, or early stopping are not applied, allowing the model to overfit.\n",
    "\n",
    "5. **Too Many Training Epochs**:\n",
    "   - Training the model for too long can lead to overfitting, as the model starts to memorize the training data instead of generalizing.\n",
    "\n",
    "6. **High Feature Dimensionality**:\n",
    "   - When the input data has too many features (dimensions) relative to the number of training samples, the model may overfit.\n",
    "\n",
    "### How to Prevent Overfitting:\n",
    "- Use **regularization techniques** like L1/L2 regularization or dropout.\n",
    "- Collect more training data if possible.\n",
    "- Use **data augmentation** to artificially increase the size of the dataset.\n",
    "- Reduce the complexity of the model (e.g., fewer layers or parameters).\n",
    "- Use **early stopping** to halt training when validation performance stops improving.\n",
    "- Apply **cross-validation** to evaluate the model's generalization ability.\n",
    "- Use simpler models if the dataset is small or noisy.\n",
    "\n",
    "Would you like me to explain any of these solutions in more detail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ways to Remove Overfitting\n",
    "\n",
    "Overfitting can be mitigated using the following techniques:\n",
    "\n",
    "1. **Regularization**:\n",
    "    - Apply L1 (Lasso) or L2 (Ridge) regularization to penalize large weights in the model.\n",
    "\n",
    "2. **Dropout**:\n",
    "    - Randomly drop neurons during training to prevent co-adaptation of neurons.\n",
    "\n",
    "3. **Early Stopping**:\n",
    "    - Stop training when the validation loss stops improving to avoid overfitting to the training data.\n",
    "\n",
    "4. **Data Augmentation**:\n",
    "    - Increase the size of the training dataset by applying transformations like rotation, flipping, cropping, etc.\n",
    "\n",
    "5. **Reduce Model Complexity**:\n",
    "    - Simplify the model by reducing the number of layers, neurons, or parameters.\n",
    "\n",
    "6. **Collect More Data**:\n",
    "    - Gather more training data to provide the model with diverse examples.\n",
    "\n",
    "7. **Cross-Validation**:\n",
    "    - Use techniques like k-fold cross-validation to evaluate the model's generalization ability.\n",
    "\n",
    "8. **Batch Normalization**:\n",
    "    - Normalize the inputs to each layer to stabilize training and act as a regularizer.\n",
    "\n",
    "9. **Ensemble Methods**:\n",
    "    - Combine predictions from multiple models to reduce variance and improve robustness.\n",
    "\n",
    "10. **Feature Selection**:\n",
    "     - Remove irrelevant or redundant features to reduce the dimensionality of the input data.\n",
    "\n",
    "By applying these techniques, you can improve the generalization of your model and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,577</span> (33.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,577\u001b[0m (33.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,577</span> (33.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,577\u001b[0m (33.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Create a simple neural network with L2 regularization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(100,), kernel_regularizer=l2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
