{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained models are essential in machine learning and deep learning because they provide a strong starting point for solving complex problems. Here's why they are needed:\n",
    "\n",
    "1. **Reduced Training Time**: Pretrained models are already trained on large datasets, so you don't need to train a model from scratch. This saves significant time and computational resources.\n",
    "\n",
    "2. **Better Performance**: Pretrained models often capture general features (like edges, shapes, or textures in images) that are transferable to other tasks. This improves the performance of your model, especially when you have limited data.\n",
    "\n",
    "3. **Overcoming Data Scarcity**: In many cases, collecting and labeling large datasets is expensive and time-consuming. Pretrained models allow you to leverage knowledge from large datasets (e.g., ImageNet, GPT) and apply it to your specific problem.\n",
    "\n",
    "4. **Fine-Tuning for Specific Tasks**: You can adapt a pretrained model to your specific task by fine-tuning it on your dataset. This combines the general knowledge of the pretrained model with the specific knowledge of your domain.\n",
    "\n",
    "5. **Standardization**: Pretrained models provide a standardized starting point, which is especially useful for benchmarking and reproducibility in research and development.\n",
    "\n",
    "For example:\n",
    "- In computer vision, models like ResNet or VGG are pretrained on ImageNet.\n",
    "- In natural language processing, models like BERT or GPT are pretrained on massive text corpora.\n",
    "\n",
    "Would you like an example of how to use a pretrained model in a specific framework like TensorFlow or PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **pretrained model** in machine learning refers to a model that has already been trained on a large dataset for a specific task, such as image classification, natural language processing, or speech recognition. Instead of training a model from scratch, you can use a pretrained model as a starting point and fine-tune it for your specific task. This approach saves time, computational resources, and often leads to better performance, especially when you have limited data.\n",
    "\n",
    "### Key Concepts of Pretrained Models:\n",
    "1. **Transfer Learning**: Pretrained models are often used in transfer learning, where knowledge gained from one task (e.g., recognizing objects in images) is applied to a different but related task (e.g., detecting specific objects in medical images).\n",
    "\n",
    "2. **Feature Extraction**: Pretrained models learn general features (e.g., edges, shapes, patterns in images, or word embeddings in text) that can be reused for other tasks.\n",
    "\n",
    "3. **Fine-Tuning**: After loading a pretrained model, you can fine-tune it by training it on your specific dataset. This adjusts the model's weights to better suit your task.\n",
    "\n",
    "4. **Popular Pretrained Models**:\n",
    "   - **Computer Vision**: Models like ResNet, VGG, Inception, and EfficientNet trained on ImageNet.\n",
    "   - **Natural Language Processing**: Models like BERT, GPT, and RoBERTa trained on large text corpora.\n",
    "\n",
    "### Example Use Case:\n",
    "If you're building an image classifier for medical X-rays but don't have a large dataset, you can use a pretrained model like ResNet (trained on ImageNet) and fine-tune it on your medical dataset. The pretrained model already knows how to detect general features, so it requires less data and training time to adapt to your specific task.\n",
    "\n",
    "Would you like an example of how to use a pretrained model in a specific framework like TensorFlow or PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/ae0df484-d135-4a56-8d55-2974988e8cd9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Applications\n",
    "![Image](https://github.com/user-attachments/assets/4af98b57-910d-4889-b5d2-e984e5a010ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m179648224/179648224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "model = ResNet101(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model.predict(x)\n",
    "    print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "Predicted: [('n02102318', 'cocker_spaniel', 0.3600198), ('n02088364', 'beagle', 0.19487344), ('n02099601', 'golden_retriever', 0.11737314)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\dogs_vs_cats\\train\\dogs\\dog.102.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "Predicted: [('n02124075', 'Egyptian_cat', 0.72881776), ('n02123159', 'tiger_cat', 0.08915132), ('n02123045', 'tabby', 0.029087849)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\dogs_vs_cats\\test\\cats\\cat.445.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "Predicted: [('n07716906', 'spaghetti_squash', 0.2034459), ('n07753113', 'fig', 0.19297945), ('n07753592', 'banana', 0.14922214)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\watermelon.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Predicted: [('n03942813', 'ping-pong_ball', 0.30494702), ('n03929660', 'pick', 0.11196379), ('n04409515', 'tennis_ball', 0.047262385)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\bdflag.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Predicted: [('n02129604', 'tiger', 0.6940935), ('n02123159', 'tiger_cat', 0.30304775), ('n02391049', 'zebra', 0.0008136297)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\tiger.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "Predicted: [('n07753592', 'banana', 0.0954813), ('n07718472', 'cucumber', 0.06836378), ('n07720875', 'bell_pepper', 0.05480585)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\tomato.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "Predicted: [('n12267677', 'acorn', 0.46520934), ('n07615774', 'ice_lolly', 0.3085524), ('n02948072', 'candle', 0.054662857)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\mango.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Predicted: [('n07714571', 'head_cabbage', 0.5098466), ('n04367480', 'swab', 0.29387206), ('n07715103', 'cauliflower', 0.024347408)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\spinch.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "Predicted: [('n03476684', 'hair_slide', 0.5577068), ('n04380533', 'table_lamp', 0.055583052), ('n03792972', 'mountain_tent', 0.050555885)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\book.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Predicted: [('n03376595', 'folding_chair', 0.8998375), ('n03903868', 'pedestal', 0.0307506), ('n04099969', 'rocking_chair', 0.020746665)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\chair.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n",
      "Predicted: [('n03903868', 'pedestal', 0.35379082), ('n04344873', 'studio_couch', 0.1769711), ('n03376595', 'folding_chair', 0.10184861)]\n"
     ]
    }
   ],
   "source": [
    "predict(r\"J:\\Dataset\\DeepLearning\\own\\table.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
