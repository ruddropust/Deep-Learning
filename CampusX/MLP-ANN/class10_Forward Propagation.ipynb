{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural networks stand as foundational architectures in deep learning. Neural networks consist of an input layer, at least one hidden layer, and an output layer. Each node is connected to nodes in the preceding and succeeding layers with corresponding weights and thresholds. In this article, we will explore what is forward propagation and it's working.\n",
    "\n",
    "## What is Forward Propagation in Neural Networks?\n",
    "\n",
    "Forward propagation is the process in a neural network where the input data is passed through the network's layers to generate an output. It involves the following steps:\n",
    "\n",
    "1. **Input Layer**: The input data is fed into the input layer of the neural network.\n",
    "\n",
    "2. **Hidden Layers**: The input data is processed through one or more hidden layers. Each neuron in a hidden layer receives inputs from the previous layer, applies an activation function to the weighted sum of these inputs, and passes the result to the next layer.\n",
    "\n",
    "3. **Output Layer**: The processed data moves through the output layer, where the final output of the network is generated. The output layer typically applies an activation function suitable for the task, such as softmax for classification or linear activation for regression.\n",
    "\n",
    "4. **Prediction**: The final output of the network is the prediction or classification result for the input data.\n",
    "\n",
    "\n",
    "Forward propagation is essential for making predictions in neural networks. It calculates the output of the network for a given input based on the current values of the weights and biases. The output is then compared to the actual target value to calculate the loss, which is used to update the weights and biases during the training process.\n",
    "\n",
    "## Mathematical Explanation of Forward Propagation\n",
    "![Image](https://github.com/user-attachments/assets/473e1911-c098-4368-97a2-8cb6baeb672e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above picture, the first layer of $X_{i1}, X_{i2}, X_{i3}, X_{i4}$ are the input layer, and the last layer of $b_{31}$ is the output layer.  \n",
    "\n",
    "Other layers are hidden layers in this structure of an ANN. This is a **4-layered deep ANN** where:  \n",
    "- The first hidden layer consists of **3 neurons**.  \n",
    "- The second hidden layer consists of **2 neurons**.  \n",
    "\n",
    "There are a total of **26 trainable parameters**.  \n",
    "In **hidden layer 1**, the top-to-bottom biases are $b_{11}, b_{12}, b_{13}$.  \n",
    "In **hidden layer 2**, the top-to-bottom biases are $b_{21}, b_{22}$.  \n",
    "The **output layer** contains the neuron having bias $b_{31}$.  \n",
    "\n",
    "Likewise, the weights of the corresponding connections are assigned as follows:  \n",
    "$W_{111}, W_{112}, W_{113}, W_{121}, W_{122}$, etc.  \n",
    "\n",
    "Here, considering we are using the **sigmoid function** as the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/193b10ff-ed37-481b-b764-6137933dcd82)\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/eab83378-e60f-4d93-ac1a-74af6948ee79)\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/1067fa6c-abd3-4b45-85fa-79477428327a)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
