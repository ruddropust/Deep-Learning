{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is transfer learning?\n",
    "--------------------------\n",
    "\n",
    "Transfer learning (TL) is a [machine learning (ML)](https://aws.amazon.com/what-is/machine-learning/) technique where a model pre-trained on one task is fine-tuned for a new, related task. Training a new ML model is a time-consuming and intensive process that requires a large amount of data, computing power, and several iterations before it is ready for production. Instead, organizations use TL to retrain existing models on related tasks with new data. For example, if a machine learning model can identify images of dogs, it can be trained to identify cats using a smaller image set that highlights the feature differences between dogs and cats.\n",
    "\n",
    "What are the benefits of transfer learning?\n",
    "-------------------------------------------\n",
    "\n",
    "TL offers several of the following benefits to researchers creating ML applications.\n",
    "\n",
    "### Enhanced efficiency\n",
    "\n",
    "Training ML models takes time as they build knowledge and identify patterns. It also requires a large data set and is computationally expensive. In TL, a pre-trained model retains fundamental knowledge of tasks, features, weights, and functions, allowing it to adapt to new tasks faster. You can use a much smaller dataset and fewer resources while achieving better results. \n",
    "\n",
    "### Increased accessibility\n",
    "\n",
    "Building deep-learning neural networks requires large data volumes, resources, computing power, and time. TL overcomes these barriers to creation, allowing organizations to adopt ML for custom use cases. You can adapt existing models to your requirements at a fraction of the cost. For example, using a pre-trained image recognition model, you can create models for medical imaging analysis, environmental monitoring, or facial recognition with minimal adjustments.\n",
    "\n",
    "### Improved performance\n",
    "\n",
    "Models developed through TL often demonstrate greater robustness in diverse and challenging environments. They better handle real-world variability and noise, having been exposed to a wide range of scenarios in their initial training. They give better results and adapt to unpredictable conditions more flexibly.\n",
    "\n",
    "What are the different transfer learning strategies?\n",
    "----------------------------------------------------\n",
    "\n",
    "The strategy you use to facilitate TL will depend on the domain of the model you are building, the task it needs to complete, and the availability of training data.\n",
    "\n",
    "### Transductive transfer learning\n",
    "\n",
    "*Transductive transfer learning* involves transferring knowledge from a specific source domain to a different but related target domain, with the primary focus being on the target domain. It is especially useful when there is little or no labeled data from the target domain.\n",
    "\n",
    "Transductive transfer learning asks the model to make predictions on target data by using previously-gained knowledge. As the target data is mathematically similar to the source data, the model finds patterns and performs faster. \n",
    "\n",
    "For example, consider adapting a sentiment analysis model trained on product reviews to analyze movie reviews. The source domain (product reviews) and the target domain (movie reviews) differ in context and specifics but share similarities in structure and language use. The model quickly learns to apply its understanding of sentiment from the product domain to the movie domain.\n",
    "\n",
    "### Inductive transfer learning\n",
    "\n",
    "Inductive transfer learning is where the source and target domains are the same, but the tasks the model must complete differ. The pre-trained model is already familiar with the source data and trains faster for new functions.\n",
    "\n",
    "An example of inductive transfer learning is in natural language processing (NLP). Models are pre-trained on a large set of texts and then fine-tuned using inductive transfer learning to specific functions like sentiment analysis. Similarly, computer vision models like VGG are pre-trained on large image datasets and then fine-tuned to develop object detection.\n",
    "\n",
    "### Unsupervised transfer learning\n",
    "\n",
    "*Unsupervised transfer learning* uses a strategy similar to inductive transfer learning to develop new abilities. However, you use this form of transfer learning when you only have unlabeled data in both the source and target domains. \n",
    "\n",
    "The model learns the common features of unlabeled data to generalize more accurately when asked to perform a target task. This method is helpful if it is challenging or expensive to obtain labeled source data.\n",
    "\n",
    "For example, consider the task of identifying different types of motorcycles in traffic images. Initially, the model is trained on a large set of unlabeled vehicle images. In this instance, the model independently determines the similarities and distinguishing features among different types of vehicles like cars, buses, and motorcycles. Next, the model is introduced to a small, specific set of motorcycle images. The model performance improves significantly compared to before.\n",
    "\n",
    "What are the steps in transfer learning?\n",
    "----------------------------------------\n",
    "\n",
    "There are three main steps when fine-tuning a machine-learning model for a new task.\n",
    "\n",
    "### Select a pre-trained model\n",
    "\n",
    "First, select a pre-trained model with prior knowledge or skills for a related task. A useful context for choosing a suitable model is to determine the source task of each model. If you understand the original tasks the model performed, you can find one that more effectively transitions to a new task.\n",
    "\n",
    "### Configure your pre-trained models\n",
    "\n",
    "After selecting your source model, configure it to pass knowledge to a model to complete the related task. There are two main methods of doing this.\n",
    "\n",
    "#### *Freeze pre-trained layers*\n",
    "\n",
    "Layers are the building blocks of neural networks. Each layer consists of a set of neurons and performs specific transformations on the input data. Weights are the parameters the network uses for decision-making. Initially set to random values, weights are adjusted during the training process as the model learns from the data.\n",
    "\n",
    "By freezing the weights of the pre-trained layers, you keep them fixed, preserving the knowledge that the [deep learning](https://aws.amazon.com/what-is/deep-learning/) model obtained from the source task.\n",
    "\n",
    "#### *Remove the last layer*\n",
    "\n",
    "In some use cases, you can also remove the last layers of the pre-trained model. In most ML architectures, the last layers are task-specific. Removing these final layers helps you reconfigure the model for new task requirements.\n",
    "\n",
    "#### *Introduce new layers*\n",
    "\n",
    "Introducing new layers on top of your pre-trained model helps you adapt to the specialized nature of the new task. The new layers adapt the model to the nuances and functions of the new requirement.\n",
    "\n",
    "### Train the model for the target domain\n",
    "\n",
    "You train the model on target task data to develop its standard output to align with the new task. The pre-trained model likely produces different outputs from those desired. After monitoring and evaluating the model's performance during training, you can adjust the hyperparameters or baseline neural network architecture to improve output further. Unlike weights, hyperparameters are not learned from the data. They are pre-set and play a crucial role in determining the efficiency and effectiveness of the training process. For example, you could adjust regularization parameters or the model's learning rates to improve its ability in relation to the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TL](https://www.ruder.io/content/images/2017/03/andrew_ng_drivers_ml_success-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/bcebc7eb-1a26-4497-96ab-6707b9610780)\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/0f0b5768-4d9d-4da2-b0b5-cc574bffab15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning works because it leverages knowledge gained from solving one problem and applies it to a different but related problem. Here's why it is effective:\n",
    "\n",
    "1. **Feature Reusability**: In deep learning, earlier layers of a model often learn general features (like edges, textures, or shapes in images), while later layers learn task-specific features. These general features are often useful across different tasks, so reusing them saves time and computational resources.\n",
    "\n",
    "2. **Reduced Data Requirements**: Training a deep neural network from scratch requires a large amount of labeled data. Transfer learning allows you to use pre-trained models, which have already been trained on massive datasets (e.g., ImageNet), reducing the need for extensive labeled data for your specific task.\n",
    "\n",
    "3. **Faster Training**: Since the model starts with pre-trained weights, it converges faster during fine-tuning compared to training from scratch. This is especially useful when computational resources are limited.\n",
    "\n",
    "4. **Domain Similarity**: Transfer learning is particularly effective when the source domain (the domain the model was pre-trained on) and the target domain (your specific task) share similarities. For example, a model trained on general image classification can be fine-tuned for medical imaging tasks.\n",
    "\n",
    "5. **Avoiding Overfitting**: When you have a small dataset, training a large model from scratch can lead to overfitting. Transfer learning mitigates this by starting with weights that already encode meaningful patterns, reducing the risk of overfitting.\n",
    "\n",
    "In summary, transfer learning works because it builds on the general knowledge encoded in pre-trained models, making it easier and faster to adapt to new tasks with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ways of Doing Transfer Learning\n",
    "-------------------------------\n",
    "\n",
    "There are several approaches to implementing transfer learning, depending on the task and the available data:\n",
    "\n",
    "1. **Feature Extraction**: \n",
    "    - Use the pre-trained model as a fixed feature extractor.\n",
    "    - Remove the final classification layer and use the output of the intermediate layers as input features for a new model.\n",
    "\n",
    "2. **Fine-Tuning**:\n",
    "    - Unfreeze some or all of the layers of the pre-trained model.\n",
    "    - Train the model on the new dataset with a lower learning rate to adjust the weights without losing the pre-trained knowledge.\n",
    "\n",
    "3. **Pre-trained Embeddings**:\n",
    "    - Use embeddings from pre-trained models (e.g., word embeddings like Word2Vec, GloVe, or BERT for NLP tasks).\n",
    "    - These embeddings capture semantic relationships and can be used as input features for downstream tasks.\n",
    "\n",
    "4. **Hybrid Approach**:\n",
    "    - Combine feature extraction and fine-tuning.\n",
    "    - Freeze some layers of the pre-trained model while fine-tuning others.\n",
    "\n",
    "5. **Domain Adaptation**:\n",
    "    - Adapt a model trained on a source domain to perform well on a target domain with limited labeled data.\n",
    "    - Techniques like adversarial training or domain-specific fine-tuning are often used.\n",
    "\n",
    "6. **Zero-Shot Learning**:\n",
    "    - Use a pre-trained model to perform tasks it has not been explicitly trained for.\n",
    "    - This is achieved by leveraging the model's generalization capabilities.\n",
    "\n",
    "Each method has its advantages and is chosen based on the similarity between the source and target tasks, the size of the target dataset, and the computational resources available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
